# ðŸ”¬ Analyses Directory

This directory contains **ad-hoc analytical queries** that are compiled by dbt but not materialized as tables or views. These analyses provide one-time investigations, exploratory queries, and business intelligence queries that don't require permanent storage.

## ðŸŽ¯ Purpose

- **Ad-Hoc Analysis**: One-time analytical queries and investigations
- **Exploratory Analysis**: Data exploration and discovery queries
- **Business Intelligence**: Complex analytical queries for reporting
- **Data Validation**: Validation queries for data quality assessment
- **Research Queries**: Experimental and research-oriented analysis

## ðŸ“ Directory Structure

```
analyses/
â”œâ”€â”€ .gitkeep                    # Placeholder file to maintain directory structure
â””â”€â”€ [future analysis files]    # Ad-hoc analytical SQL files
```

## ðŸ” Analysis Categories

### Business Intelligence Analyses
**Purpose**: Complex analytical queries for business insights

#### Potential Analysis Types
- **Customer Lifetime Value**: CLV calculations and segmentation
- **Cohort Analysis**: Customer behavior over time
- **Market Basket Analysis**: Subscription pattern analysis
- **Churn Prediction**: Customer retention analysis
- **Revenue Attribution**: Revenue source analysis

### Data Exploration Analyses
**Purpose**: Exploratory data analysis and discovery

#### Potential Analysis Types
- **Data Profiling**: Statistical analysis of datasets
- **Correlation Analysis**: Relationship discovery between variables
- **Outlier Detection**: Identification of data anomalies
- **Trend Analysis**: Time-series pattern discovery
- **Segmentation Research**: Customer and operational segmentation

### Validation Analyses
**Purpose**: Data quality and business logic validation

#### Potential Analysis Types
- **Data Quality Assessment**: Comprehensive data quality checks
- **Business Rule Validation**: Complex business logic verification
- **Cross-System Reconciliation**: Data consistency across systems
- **Historical Validation**: Trend and pattern validation
- **Performance Benchmarking**: System performance analysis

## ðŸ”§ Analysis Development

### Creating Analysis Files
```sql
-- Example: customer_lifetime_value_analysis.sql
{{ config(materialized='ephemeral') }}

WITH customer_metrics AS (
    SELECT 
        customer_id,
        customer_type,
        COUNT(subscription_id) as total_subscriptions,
        SUM(subscription_value) as total_revenue,
        MIN(start_date) as first_subscription,
        MAX(end_date) as last_subscription
    FROM {{ ref('slv_subscriptions') }}
    GROUP BY 1, 2
),

clv_calculation AS (
    SELECT 
        *,
        total_revenue / NULLIF(total_subscriptions, 0) as avg_subscription_value,
        DATE_DIFF('month', first_subscription, last_subscription) as customer_lifetime_months
    FROM customer_metrics
)

SELECT 
    customer_type,
    AVG(total_revenue) as avg_clv,
    MEDIAN(total_revenue) as median_clv,
    COUNT(*) as customer_count
FROM clv_calculation
GROUP BY 1
ORDER BY 2 DESC
```

### Execution Pattern
```bash
# Compile analysis (check syntax)
dbt compile --select analyses/

# Run specific analysis
dbt compile --select analyses/customer_lifetime_value_analysis

# Generate and view compiled SQL
cat target/compiled/drivesight/analyses/customer_lifetime_value_analysis.sql
```

## ðŸ“Š Analysis Workflow

### Development Process
1. **Define Question**: Clearly articulate the business question
2. **Design Approach**: Plan the analytical methodology
3. **Write Query**: Develop the SQL analysis
4. **Test Logic**: Validate query logic and results
5. **Document Findings**: Record insights and conclusions

### Execution Process
1. **Compile Query**: Use dbt compile to generate final SQL
2. **Execute Analysis**: Run compiled SQL in database
3. **Analyze Results**: Interpret and validate findings
4. **Document Insights**: Record business insights and recommendations
5. **Share Results**: Communicate findings to stakeholders

### Iteration Process
1. **Review Results**: Assess initial findings
2. **Refine Questions**: Adjust analytical questions based on results
3. **Modify Approach**: Update analytical methodology
4. **Re-execute**: Run refined analysis
5. **Validate Conclusions**: Confirm findings and insights

## ðŸ§ª Quality Assurance

### Query Validation
- **Syntax Check**: Use dbt compile to validate SQL syntax
- **Logic Review**: Peer review of analytical logic
- **Result Validation**: Validate results against known benchmarks
- **Performance Testing**: Ensure queries execute efficiently

### Business Logic Validation
- **Business Rule Compliance**: Ensure analysis follows business rules
- **Metric Definitions**: Validate metric calculations and definitions
- **Segmentation Logic**: Verify customer and operational segmentation
- **Temporal Logic**: Validate time-based calculations and trends

## ðŸ“ˆ Common Analysis Patterns

### Customer Analytics
```sql
-- Customer segmentation analysis
WITH customer_segments AS (
    SELECT 
        customer_id,
        customer_type,
        subscription_count,
        total_revenue,
        CASE 
            WHEN total_revenue >= 10000 THEN 'High Value'
            WHEN total_revenue >= 5000 THEN 'Medium Value'
            ELSE 'Low Value'
        END as value_segment
    FROM customer_summary
)
SELECT value_segment, COUNT(*), AVG(total_revenue)
FROM customer_segments
GROUP BY 1
```

### Fleet Analytics
```sql
-- Fleet utilization analysis
WITH daily_fleet AS (
    SELECT 
        date,
        city,
        active_fleet_size,
        total_subscriptions,
        active_fleet_size / NULLIF(total_subscriptions, 0) as utilization_rate
    FROM fleet_daily_summary
)
SELECT 
    city,
    AVG(utilization_rate) as avg_utilization,
    MIN(utilization_rate) as min_utilization,
    MAX(utilization_rate) as max_utilization
FROM daily_fleet
GROUP BY 1
ORDER BY 2 DESC
```

### Operational Analytics
```sql
-- Delivery performance analysis
WITH delivery_metrics AS (
    SELECT 
        city,
        week,
        delivery_volume,
        LAG(delivery_volume) OVER (PARTITION BY city ORDER BY week) as prev_week_volume
    FROM weekly_delivery_summary
)
SELECT 
    city,
    AVG((delivery_volume - prev_week_volume) / NULLIF(prev_week_volume, 0)) as avg_growth_rate,
    COUNT(*) as weeks_analyzed
FROM delivery_metrics
WHERE prev_week_volume IS NOT NULL
GROUP BY 1
ORDER BY 2 DESC
```

## ðŸ”„ Best Practices

### Development
- **Clear Naming**: Use descriptive file names that indicate analysis purpose
- **Documentation**: Include comments explaining analytical logic
- **Modularity**: Break complex analyses into logical components
- **Reusability**: Design analyses that can be easily modified for similar questions

### Execution
- **Performance**: Optimize queries for large datasets
- **Resource Management**: Consider resource usage for complex analyses
- **Validation**: Always validate results against business expectations
- **Documentation**: Record methodology and findings

### Collaboration
- **Knowledge Sharing**: Share analytical techniques and insights
- **Peer Review**: Review complex analyses with team members
- **Version Control**: Track analysis changes and iterations
- **Business Communication**: Translate technical findings into business insights

## ðŸ“Š Usage Guidelines

### When to Use Analyses
- **One-Time Questions**: Business questions that don't require ongoing monitoring
- **Exploratory Research**: Data exploration and hypothesis testing
- **Complex Calculations**: Analyses too complex for standard models
- **Validation Studies**: Data quality and business logic validation

### When to Create Models Instead
- **Recurring Needs**: Analysis needed regularly should become a model
- **Dashboard Requirements**: Data needed for dashboards should be modeled
- **Downstream Dependencies**: Data used by other models should be materialized
- **Performance Requirements**: Frequently accessed data should be pre-computed

### Migration Path
- **Prototype in Analyses**: Start with analysis file for exploration
- **Validate Business Value**: Confirm ongoing business need
- **Convert to Model**: Move valuable analyses to appropriate model layer
- **Optimize Performance**: Optimize materialization strategy for production use

## ðŸš¨ Monitoring and Maintenance

### Analysis Health
- **Execution Success**: Monitor analysis compilation and execution
- **Performance**: Track analysis execution time and resource usage
- **Usage Patterns**: Understand which analyses are most valuable
- **Business Impact**: Assess business value of analytical insights

### Lifecycle Management
- **Regular Review**: Periodically review analysis inventory
- **Archive Unused**: Archive or remove unused analyses
- **Promote Valuable**: Convert valuable analyses to permanent models
- **Update Dependencies**: Ensure analyses work with model changes

This analyses directory provides a flexible workspace for exploratory and ad-hoc analytical work, supporting data-driven decision making while maintaining the integrity of the core data models.
